# ğŸ¯ Multi-modal Online Advertisement Classification

![Python](https://img.shields.io/badge/Python-3.9%2B-blue?logo=python&logoColor=white)
![PyTorch](https://img.shields.io/badge/PyTorch-EE4C2C?logo=pytorch&logoColor=white)
![HuggingFace](https://img.shields.io/badge/HuggingFace-Transformers-F9AB00?logo=huggingface&logoColor=white)
![Flask](https://img.shields.io/badge/Web_App-Flask/Streamlit-000000?logo=flask&logoColor=white)

## ğŸ“Œ Project Overview
This project implements a robust multi-modal AI system designed to classify online advertisements into **20 distinct categories**. By leveraging both visual and textual data (images, generated captions, extracted text via OCR, and metadata), the system achieves deep semantic understanding of complex ad content.

<img width="1166" height="937" alt="image" src="https://github.com/user-attachments/assets/f6c478bf-dc3c-4cbd-b3ec-20b696a829be" />

### âœ¨ Key Highlights & Performance
- **State-of-the-Art Models:** Integrates **CLIP**, **ViLT**, and **BLIP-2** for advanced feature extraction and multimodal fusion.
- **High Accuracy:** The fine-tuned CLIP model achieved an impressive **84.36% accuracy** on a rigorous test dataset of 5,929 samples.
- **Microservices Architecture:** The web demonstration is built using a decoupled architecture with independent services for Optical Character Recognition (OCR), Text Generation (Caption/Slogan), and the Main User Interface.

---

## ğŸ“‚ Repository Structure

```text
source_code/
â”œâ”€â”€ Source/
â”‚   â”œâ”€â”€ Training/                 # Jupyter Notebooks for model fine-tuning
â”‚   â”‚   â”œâ”€â”€ clip_qa_cap_slo_ocr.ipynb
â”‚   â”‚   â””â”€â”€ vilt_qa_cap_slo_ocr.ipynb
â”‚   â”‚
â”‚   â””â”€â”€ App_Demo/                 # Web Application & Microservices
â”‚       â”œâ”€â”€ checkpoints/            # Pre-trained model weights (.pth)
â”‚       â”œâ”€â”€ gen_service/            # BLIP-2 Generation Service
â”‚       â”œâ”€â”€ ocr_service/            # PaddleOCR Service
â”‚       â””â”€â”€ main_app/               # Main UI and Inference integration
â”œâ”€â”€ requirements_blip_env.txt
â”œâ”€â”€ requirements_paddleocr_env.txt
â””â”€â”€ requirements_torch_env.txt
```

---

## âš™ï¸ Environment Setup
To avoid dependency conflicts, this project requires three separate Conda virtual environments.

**1. OCR Service Environment (Requires Python 3.9)**

```bash
conda create -n env_ocr python=3.9
conda activate env_ocr
pip install -r requirements_paddleocr_env.txt
```

**2. Generation Service Environment (BLIP)**

```bash
conda create -n env_blip python=3.10
conda activate env_blip
pip install -r requirements_blip_env.txt
```

**3. Main App & PyTorch Environment (CLIP/ViLT)**

```bash
conda create -n env_main python=3.10
conda activate env_main
pip install -r requirements_torch_env.txt
```
---

## ğŸš€ Running the Web Demo
The application runs on three parallel services. Open three separate terminals (Anaconda Prompts), navigate to `source_code/Source/App_Demo`, and run the following commands simultaneously:

**Terminal 1: Start OCR Service**

```bash
conda activate env_ocr
cd ocr_service
python server.py
```

**Terminal 2: Start Generation Service**

```bash
conda activate env_blip
cd gen_service
python server.py
```

**Terminal 3: Start Main App UI**

```bash
conda activate env_main
cd main_app
python app.py
```

Once all services are running, open your web browser and navigate to the local URL provided in Terminal 3 (e.g., *`http://127.0.0.1:5000`*) to access the interactive demo.

---

## ğŸ‘¨â€ğŸ’» Authors
- [PhD. Luong Thi Ngoc Khanh] - Instructor
- [Tran Ba Dat] - Computer Science Student at Ton Duc Thang University
- [Nguyen Tan Phong] - Computer Science Student at Ton Duc Thang University

*This project was developed as part of the graduation thesis/coursework.*
